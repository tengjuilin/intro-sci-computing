{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconstrained Optimization\n",
    "\n",
    "Teng-Jui Lin\n",
    "\n",
    "Content adapted from UW AMATH 301, Beginning Scientific Computing, in Spring 2020.\n",
    "\n",
    "- Unconstrained optimization methods\n",
    "    - Derivative-free method\n",
    "        - Golden section search\n",
    "    - Derivative method\n",
    "        - Gradient descent\n",
    "- `scipy` implementations\n",
    "    - Minimize scalar functions by [`scipy.optimize.fminbound()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fminbound.html)\n",
    "    - Minimize multivariable functions by [`scipy.optimize.fmin()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden section search\n",
    "\n",
    "Goal: find the minimum of a single-variable unimodal function $f(x)$.\n",
    "\n",
    "- unconstrained\n",
    "- derivative-free\n",
    "\n",
    "In a given interval $[a, b]$, we find two points $x_1$ and $x_2$ where $a<x_1<x_2<b$. The two points can be compared to determine the new interval:\n",
    "\n",
    "- If $f(x_1) \\le f(x_2)$, then change the new interval to $[a, x_2]$\n",
    "- If $f(x_1) > f(x_2)$, then change the new interval to $[x_1, b]$\n",
    "\n",
    "The golden section search minimizes the number of calculation rather than iteration, so we want to\n",
    "\n",
    "- have a constant reduction factor $c$\n",
    "  - If $f(x_1) \\le f(x_2)$, $c = \\dfrac{x_2 - a}{b-a} \\implies x_2 = (1-c)a + cb$\n",
    "  - If $f(x_1) > f(x_2)$, $c = \\dfrac{b-x_1}{b-a} \\implies x_1 = ca + (1-c)b$\n",
    "- reuse calculated points\n",
    "  - $x_{2}^{\\text {new}}=x_{1}^{\\text {old}}=c a+(1-c) b$\n",
    "  - $x_{2}^{\\text {new}}=(1-c) a+c x_{2}^{\\text {old}}=(1-c) a+c[(1-c) a+c b]$\n",
    "\n",
    "Equate the above two equations, we get\n",
    "\n",
    "$$\n",
    "c^2 + c + 1 = 0 \\\\\n",
    "c = \\dfrac{\\sqrt{5} - 1}{2} \\approx 0.618\n",
    "$$\n",
    "\n",
    "Therefore, we can reuse the points during iteration by choosing:\n",
    "  - If $f(x_1) \\le f(x_2)$\n",
    "    - $x_1^{\\text{new}} = x_2^{\\text{old}}$\n",
    "    - $x_2^{\\text{new}} = (1-c)a + cb$\n",
    "  - If $f(x_1) > f(x_2)$\n",
    "    - $x_2^{\\text{new}} = x_1^{\\text{old}}$\n",
    "    - $x_1^{\\text{new}} = ca + (1-c)b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "**Problem Statement.** Find the minimum of the function \n",
    "\n",
    "$$\n",
    "f(x) = -\\dfrac{1}{3}e^{-x/2} - 3xe^{-x/2}\n",
    "$$\n",
    "\n",
    "using golden section search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden_section_search(f, a, b, tolerance=1e-6, max_iter=5000):\n",
    "    '''\n",
    "    Find the minimum of single-variable unimodal function using golden section search method.\n",
    "    \n",
    "    :param f: objective function\n",
    "    :param a: lower bound of interval\n",
    "    :param b: upper bound of interval\n",
    "    :param tolerance: tolerance of stopping criteria\n",
    "    :param max_iter: maximum iterations allowed for calculation\n",
    "    :returns: minimum of objective function\n",
    "    '''\n",
    "    c = (np.sqrt(5) - 1)/2\n",
    "    x1 = c*a + (1 - c)*b\n",
    "    x2 = (1 - c)*a + c*b\n",
    "    f1 = f(x1)\n",
    "    f2 = f(x2)\n",
    "    \n",
    "    i = 0  # iteration counter\n",
    "    \n",
    "    # golden section search logic\n",
    "    while (b-a) > tolerance and i < max_iter:\n",
    "        if f1 <= f2:\n",
    "            b = x2\n",
    "            x2 = x1\n",
    "            f2 = f1\n",
    "            x1 = c*a + (1 - c)*b\n",
    "            f1 = f(x1)\n",
    "        else:\n",
    "            a = x1\n",
    "            x1 = x2\n",
    "            f1 = f2\n",
    "            x2 = (1 - c)*a + c*b\n",
    "            f2 = f(x2)\n",
    "    \n",
    "    # maximum iteration warning\n",
    "    if i == max_iter:\n",
    "        import warnings\n",
    "        warnings.warn(f'Maximum iteration reached. Current stopping criteria is {b-a}', UserWarning)\n",
    "    \n",
    "    result = (a+b)/2  # take avg of upper and lower bound\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(x):\n",
    "    return -(1/3*np.exp(-x/2) + 3*x*np.exp(-x/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.888888807141935"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0\n",
    "b = 10\n",
    "golden_section_search(test_func, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding minimum using `scipy.optimize.fminbound()`\n",
    "\n",
    "[`scipy.optimize.fminbound()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fminbound.html) find the minimum for scalar function in given interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8888887480521486"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fminbound(test_func, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Goal: find the minimum of a multi-variable function $f(\\mathbf{x})$\n",
    "\n",
    "- unconstrained\n",
    "- derivative method\n",
    "\n",
    "Gradient descent chooses the next point in the steepest downhill direction, where the function value is minimum. For each guess $\\mathbf{p_n}$, we calculate $\\mathbf{p_{n+1}}$ by\n",
    "\n",
    "1. Calculate $\\nabla f(\\mathbf{p_n})$\n",
    "2. Define $\\phi(t) = \\mathbf{p_n} - t\\nabla f(\\mathbf{p_n})$\n",
    "3. Define $f(\\phi(t))$\n",
    "4. Find $t^*$ using `fminbound()` to minimize $f(\\phi(t))$\n",
    "5. Define $\\mathbf{p_{n+1}} = \\phi(t^*)$\n",
    "\n",
    "Optimization is the speed limiting step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "**Problem Statement.** Find the minimum of the function\n",
    "\n",
    "$$\n",
    "f(x, y) = (x-2)^2 + (y+1)^2 + 5\\sin x \\sin y + 100\n",
    "$$\n",
    "\n",
    "using gradient descent and compare with the result from `scipy.optimize.fmin()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, fgrad, x0, tolerance=1e-6, max_iter=10000):\n",
    "    '''\n",
    "    Find a minimum of multivariable function by gradient descent.\n",
    "    \n",
    "    :param f: objective function\n",
    "    :param fgrad: gradient of objective function\n",
    "    :param x0: initial guess (starting point)\n",
    "    :param tolerance: tolerance of stopping criteria\n",
    "    :param max_iter: maximum iterations allowed for calculation\n",
    "    :returns: minimum of objective function\n",
    "    '''\n",
    "    import scipy\n",
    "    from scipy import optimize\n",
    "    \n",
    "    i = 0  # iteration counter\n",
    "    x = x0\n",
    "    grad = fgrad(x)\n",
    "    \n",
    "    # gradient descent logic\n",
    "    while np.linalg.norm(grad, np.inf) > tolerance and i < max_iter:\n",
    "        grad = fgrad(x)\n",
    "        phi = lambda t : x - t*grad\n",
    "        f_of_phi = lambda t : f(phi(t))\n",
    "        tmin = scipy.optimize.fminbound(f_of_phi, 0, 1)\n",
    "        x = phi(tmin)  # use next point with minimum function value\n",
    "        i += 1\n",
    "    \n",
    "    # maximum iteration warning\n",
    "    if i == max_iter:\n",
    "        import warnings\n",
    "        warnings.warn(f'Maximum iteration reached. Current stopping criteria is {np.linalg.norm(grad, np.inf)}', UserWarning)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxy = lambda x, y : (x - 2)**2 + (y + 1)**2 + 5*np.sin(x)*np.sin(y) + 100\n",
    "f = lambda p : fxy(*p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgradx = lambda x, y : 2*(x - 2) + 5*np.cos(x)*np.sin(y) \n",
    "fgrady = lambda x, y : 2*(y + 1) + 5*np.sin(x)*np.cos(y)\n",
    "fgrad =  lambda p : np.array([fgradx(*p), fgrady(*p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.69484631, -1.40628341])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = [6, 4]\n",
    "gradient_descent(f, fgrad, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 95.363597\n",
      "         Iterations: 42\n",
      "         Function evaluations: 82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.69487388, -1.40630558])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare with scipy\n",
    "scipy.optimize.fmin(f, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with fixed steps\n",
    "\n",
    "Goal: find the minimum of a multi-variable function $f(\\mathbf{x})$\n",
    "\n",
    "- unconstrained\n",
    "- derivative method\n",
    "\n",
    "Gradient descent chooses the next point in the steepest downhill direction, with a constant learning rate (fixed step) $t$. For each guess $\\mathbf{p_n}$, we calculate $\\mathbf{p_{n+1}}$ by\n",
    "\n",
    "1. Calculate $\\nabla f(\\mathbf{p_n})$\n",
    "2. Define $\\mathbf{p_{n+1}} = \\mathbf{p_{n}} - t\\nabla f(\\mathbf{p_n})$\n",
    "\n",
    "Calculating gradient is the speed limiting step. The learning rate need to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "**Problem Statement.** Find the minimum of the function\n",
    "\n",
    "$$\n",
    "f(x, y) = (x-2)^2 + (y+1)^2 + 5\\sin x \\sin y + 100\n",
    "$$\n",
    "\n",
    "using gradient descent with fixed steps and compare with the result from `scipy.optimize.fmin()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_fixed_step(f, fgrad, x0, learning_rate, tolerance=1e-6, max_iter=10000):\n",
    "    '''\n",
    "    Find a minimum of multivariable function by gradient descent.\n",
    "    \n",
    "    :param f: objective function\n",
    "    :param fgrad: gradient of objective function\n",
    "    :param x0: initial guess (starting point)\n",
    "    :param learning_rate: learning rate\n",
    "    :param tolerance: tolerance of stopping criteria\n",
    "    :param max_iter: maximum iterations allowed for calculation\n",
    "    :returns: minimum of objective function\n",
    "    '''\n",
    "    import scipy\n",
    "    from scipy import optimize\n",
    "    \n",
    "    i = 0  # iteration counter\n",
    "    x = x0\n",
    "    grad = fgrad(x)\n",
    "    \n",
    "    # gradient descent with fixed step logic\n",
    "    while np.linalg.norm(grad, np.inf) > tolerance and i < max_iter:\n",
    "        grad = fgrad(x)\n",
    "        x = x - learning_rate*grad  # use next point with fixed step\n",
    "        i += 1\n",
    "    \n",
    "    # maximum iteration warning\n",
    "    if i == max_iter:\n",
    "        import warnings\n",
    "        warnings.warn(f'Maximum iteration reached. Current stopping criteria is {np.linalg.norm(grad, np.inf)}', UserWarning)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.69484629, -1.40628343])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = [6, 4]\n",
    "gradient_descent_fixed_step(f, fgrad, x0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 95.363597\n",
      "         Iterations: 42\n",
      "         Function evaluations: 82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.69487388, -1.40630558])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare with scipy\n",
    "scipy.optimize.fmin(f, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding minimum using `scipy.optimize.fmin()`\n",
    "\n",
    "[`scipy.optimize.fmin()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html) finds a minimum for multivariable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 95.363597\n",
      "         Iterations: 42\n",
      "         Function evaluations: 82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.69487388, -1.40630558])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin(f, x0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
